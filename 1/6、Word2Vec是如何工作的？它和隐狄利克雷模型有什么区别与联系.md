Word2Vec是如何工作的？它和隐狄利克雷模型有什么区别与联系



谷歌 2013年提出的 Word2Vec 是目前最常用的词嵌入模型之一。Word2Vec实际是一种浅层的神经网络模型，有两种网络结构：CBOW（连续词袋）和Skip-gram

CBOW的目标是根据上下文出现的词，预测当前词的生成概率。而 Skip-gram 是根据当前词预测上下文中各词的生成概率。

![](C:\Users\linky\Documents\study\job\project\baimian\data\1\assets\a.png)

CBOW和Skip-gram 都可以表示成由**输入层**、**映射层**和**输出层**组成的浅层神经网络

输入中的每个词由独热编码表示，所有词均表示成一个N维向量，N 为词汇表中单词的总数。映射层是一个 $N\times K$ 维的权重矩阵，输出层则是一个 $K\times N$ 维的权重矩阵。输出层也是一个 N 维向量，最后对输出层向量引用 softmax 激活函数。但是由于 softmax激活函数中存在归一化项的缘故，推导出来的公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢。由此产生了 Hierarchical Softmax和 Negative Sampling 两种改进方法。

**Word2Vec 与 LDA 的区别和联系**

LDA 属于主题模型，而主题模型是一种基于**概率图模型**的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量。

Word2Vec属于词嵌入模型，词嵌入模型一般表达为**神经网络**的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量。

LDA可以理解为对 "文档 - 单词" 矩阵进行分解，得到 ”文档 - 主题“ 和 ”主题 - 单词“ 两个概率分布。

Word2Vec是对 ”上下文 - 单词“ 矩阵进行学习，其中上下文由周围的几个单词组成。由此得到的词向量表示更多地融入了上下文共现的特征。



